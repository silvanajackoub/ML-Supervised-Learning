{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jgz87dCgld-5"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8z1tKillvs4",
        "outputId": "7629c01c-0b89-4d38-aab7-f09fe7696f9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: (70000, 784)\n",
            "y shape: (70000,)\n"
          ]
        }
      ],
      "source": [
        "# Load the MNIST dataset using fetch_openml\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "\n",
        "# Extract the features (pixel values) and labels from the dataset\n",
        "X = mnist.data.values.astype('float32')\n",
        "y = mnist.target.values.astype('int64')\n",
        "\n",
        "# Print the shape of the data arrays\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LtmsOBebnthJ"
      },
      "outputs": [],
      "source": [
        "# Subset data to use only class 0 and class 1\n",
        "X = X[(y == 0) | (y == 1)]\n",
        "y = y[(y == 0) | (y == 1)]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Standardize Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i4opo_ggl0zP"
      },
      "outputs": [],
      "source": [
        "# Define a small epsilon value to add to the standard deviation to avoid division by zero\n",
        "eps = 1e-8\n",
        "\n",
        "# Calculate the standard deviation of each feature and replace any zero values with eps\n",
        "std_dev = np.std(X, axis=0)\n",
        "std_dev[std_dev == 0] = eps\n",
        "\n",
        "# Normalize the data by subtracting the mean and dividing by the standard deviation\n",
        "X = (X - np.mean(X, axis=0)) / std_dev"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M8AMoIAwjMRw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(X , y ,test_size=0.2, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implement Logistic Regression with different optimizers and L1 values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HEjn__Ekl768"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, lr=0.01, n_iters=1000 , l1=0 , batch_size = None, optimizer='sgd'):\n",
        "\n",
        "      self.lr = lr\n",
        "      self.n_iters = n_iters\n",
        "      self.weights = None\n",
        "      self.bias = None\n",
        "      self.l1 = l1\n",
        "      self.batch_size = batch_size\n",
        "      self.optimizer = optimizer\n",
        "      self.m_w = None  # for RMSProp and Adam optimizers\n",
        "      self.v_w = None  # for Adam optimizer\n",
        "      \n",
        "\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "\n",
        "      n_samples, n_features = X.shape\n",
        "      \n",
        "      # Initialize the weights and bias to zeros\n",
        "      self.weights = np.zeros(n_features)\n",
        "      self.bias = 0\n",
        "\n",
        "\n",
        "      if self.optimizer == 'RMSprop':\n",
        "        self.m_w = np.zeros(n_features)\n",
        "      elif self.optimizer == 'Adam':\n",
        "        self.m_w = np.zeros(n_features)\n",
        "        self.v_w = np.zeros(n_features)\n",
        "      \n",
        "      if self.batch_size is None:\n",
        "        self.batch_size = 1  # set batch_size to 1 for stochastic gradient descent\n",
        "\n",
        "      n_batches = n_samples // self.batch_size\n",
        "\n",
        "      \n",
        "      # Gradient descent\n",
        "      for i in range(self.n_iters):\n",
        "        \n",
        "        # Randomly sample a batch of examples\n",
        "        indices = np.random.permutation(n_samples)\n",
        "        X = X[indices]\n",
        "        y = y[indices]\n",
        "\n",
        "        for j in range(n_batches):\n",
        "            start = j * self.batch_size\n",
        "            end = start + self.batch_size\n",
        "            x_batch = X[start:end]\n",
        "            y_batch = y[start:end]\n",
        "\n",
        "            # Calculate the linear model using dot product of weights and features, and add the bias\n",
        "            linear_model = np.dot(x_batch , self.weights) + self.bias\n",
        "            \n",
        "            # Apply sigmoid function to get the predicted probabilities\n",
        "            y_predicted = self._sigmoid(linear_model)\n",
        "            \n",
        "            # Calculate the gradient of the cost function using the predicted probabilities and true labels\n",
        "            dw = (1/self.batch_size) * np.dot(x_batch.T, (y_predicted - y_batch))\n",
        "            db = (1/self.batch_size) * np.sum(y_predicted - y_batch)\n",
        "      \n",
        "  \n",
        "        # Add L1 regularization to the gradient\n",
        "        if self.l1:\n",
        "          dw += self.l1 * self.weights\n",
        "        \n",
        "          \n",
        "        # Update the weights and bias using the selected optimizer\n",
        "        if self.optimizer == 'sgd':\n",
        "\n",
        "          self.weights -= self.lr * dw\n",
        "          self.bias -= self.lr *db\n",
        "        \n",
        "        elif self.optimizer == 'RMSprop':\n",
        "          \n",
        "          # Update moving average of squared gradients\n",
        "          self.m_w = 0.9 * self.m_w + 0.1 * (dw ** 2)\n",
        "          \n",
        "          # Update the weights and bias using RMSProp optimizer\n",
        "          self.weights -= self.lr * dw / (np.sqrt(self.m_w) + 1e-8)\n",
        "          self.bias -= self.lr * db\n",
        "\n",
        "        elif self.optimizer == 'Adam':\n",
        "          self.m_w = 0.9 * self.m_w + 0.1 * dw\n",
        "          self.v_w = 0.999 * self.v_w + 0.001 * (dw ** 2)\n",
        "\n",
        "          # Compute bias-corrected first and second moment estimates\n",
        "          m_w_corrected = self.m_w / (1 - 0.9 ** (i+1))\n",
        "          v_w_corrected = self.v_w / (1 - 0.999 ** (i+1))\n",
        "          \n",
        "          # Update the weights and bias using Adam optimizer\n",
        "          self.weights -= self.lr * m_w_corrected / (np.sqrt(v_w_corrected) + 1e-8)\n",
        "          self.bias -= self.lr * db\n",
        "    \n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._sigmoid(linear_model)\n",
        "        return np.where(y_predicted > 0.5, 1, 0)\n",
        "    \n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7-hSd27H9vqk"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters\n",
        "lr = 0.001\n",
        "batch_list =[32 , 64 , 128 ]\n",
        "lambda_list =[1 , 0.1 , 0.001]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use L1 regularization with gradient descent optimizer. Try 2 values for lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLgsadnSX6vZ",
        "outputId": "82350d2f-78be-4074-bcf7-a1a1bfe138b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lambda: 1, train accuracy: 0.9966, test accuracy: 0.9976\n",
            "Lambda: 0.1, train accuracy: 0.9959, test accuracy: 0.9976\n",
            "Lambda: 0.001, train accuracy: 0.9964, test accuracy: 0.9970\n"
          ]
        }
      ],
      "source": [
        "for lam in lambda_list:\n",
        "\n",
        "  # Initialize the model\n",
        "  model = LogisticRegression(lr = lr, l1 = lam)\n",
        "  # Train the model\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Evaluate the model on the training and testing sets\n",
        "  train_acc = np.mean(model.predict(X_train) == y_train)\n",
        "  test_acc = np.mean(model.predict(X_test) == y_test)\n",
        "\n",
        "  # Print the results\n",
        "  print(f\"Lambda: {lam}, train accuracy: {train_acc:.4f}, test accuracy: {test_acc:.4f}\")\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use mini-batch gradient descent optimizer. Try multiple batches (at least 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZN_RPQC1NRx",
        "outputId": "bfc8d343-3475-42a1-a728-a228179c548a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 32, train accuracy: 0.9988, test accuracy: 0.9990\n",
            "Batch size: 64, train accuracy: 0.9987, test accuracy: 0.9990\n",
            "Batch size: 128, train accuracy: 0.9987, test accuracy: 0.9990\n"
          ]
        }
      ],
      "source": [
        "for batch_size in batch_list:\n",
        "  \n",
        "  # Initialize the model\n",
        "  model1 = LogisticRegression(lr = lr, batch_size = batch_size)\n",
        "  # Train the model\n",
        "  model1.fit(X_train, y_train)\n",
        "\n",
        "  # Evaluate the model on the training and testing sets\n",
        "  train_acc = np.mean(model1.predict(X_train) == y_train)\n",
        "  test_acc = np.mean(model1.predict(X_test) == y_test)\n",
        "\n",
        "  print(f\"Batch size: {batch_size}, train accuracy: {train_acc:.4f}, test accuracy: {test_acc:.4f}\")\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use RMS Prop optimizer and Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNEa7XwN2kJr",
        "outputId": "c5c8b9b9-ae13-4520-eaea-ec47854d9138"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizer: RMSprop, train accuracy: 0.9936, test accuracy: 0.9932\n",
            "Optimizer: Adam, train accuracy: 0.9951, test accuracy: 0.9970\n"
          ]
        }
      ],
      "source": [
        "for optimizer in ['RMSprop', 'Adam']:\n",
        "\n",
        "  # Initialize the model\n",
        "  model = LogisticRegression(lr = lr, optimizer = optimizer)\n",
        "\n",
        "  # Train the model\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Evaluate the model on the training and testing sets\n",
        "  train_acc = np.mean(model.predict(X_train) == y_train)\n",
        "  test_acc = np.mean(model.predict(X_test) == y_test)\n",
        "\n",
        "  # Print the results\n",
        "  print(f\"Optimizer: {optimizer}, train accuracy: {train_acc:.4f}, test accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use RMS Prop optimizer and Adam optimizer using mini batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy9EUQWFwBI8",
        "outputId": "08426826-8698-442c-fb9e-34379a89a77f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizer: RMSprop, train accuracy: 0.9970, test accuracy: 0.9963\n",
            "Optimizer: Adam, train accuracy: 0.9989, test accuracy: 0.9980\n"
          ]
        }
      ],
      "source": [
        "batch_size1 = 64\n",
        "\n",
        "for optimizer in ['RMSprop', 'Adam']:\n",
        "\n",
        "  # Initialize the model\n",
        "  model = LogisticRegression(lr = lr, optimizer = optimizer ,batch_size = batch_size1)\n",
        "\n",
        "  # Train the model\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Evaluate the model on the training and testing sets\n",
        "  train_acc = np.mean(model.predict(X_train) == y_train)\n",
        "  test_acc = np.mean(model.predict(X_test) == y_test)\n",
        "\n",
        "  # Print the results\n",
        "  print(f\"Optimizer: {optimizer}, train accuracy: {train_acc:.4f}, test accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and test the model using different optimizers and regularization parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI3zwdE94SeZ",
        "outputId": "508948bb-8175-48e9-b7f7-7570f6233d4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizer: sgd, lambda: 0.01, train accuracy: 0.9967, test accuracy: 0.9976\n",
            "Optimizer: sgd, lambda: 0.1, train accuracy: 0.9968, test accuracy: 0.9976\n",
            "Optimizer: RMSprop, lambda: 0.01, train accuracy: 0.9989, test accuracy: 0.9997\n",
            "Optimizer: RMSprop, lambda: 0.1, train accuracy: 0.9975, test accuracy: 0.9990\n",
            "Optimizer: Adam, lambda: 0.01, train accuracy: 0.9990, test accuracy: 0.9997\n",
            "Optimizer: Adam, lambda: 0.1, train accuracy: 0.9983, test accuracy: 0.9993\n"
          ]
        }
      ],
      "source": [
        "# Define the hyperparameters\n",
        "lambda1 = 0.01\n",
        "lambda2 = 0.1\n",
        "\n",
        "for optimizer in ['sgd', 'RMSprop', 'Adam']:\n",
        "    for lam in [lambda1, lambda2]:\n",
        "        # Initialize the model\n",
        "        model = LogisticRegression(lr = lr, optimizer = optimizer, l1 = lam, batch_size = batch_size1)\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model on the training and testing sets\n",
        "        train_acc = np.mean(model.predict(X_train) == y_train)\n",
        "        test_acc = np.mean(model.predict(X_test) == y_test)\n",
        "\n",
        "        # Print the results\n",
        "        print(f\"Optimizer: {optimizer}, lambda: {lam}, train accuracy: {train_acc:.4f}, test accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MDkDGgH34ici"
      },
      "source": [
        "###  **Here's a summary of the accuracies for each case and a brief explanation:**\n",
        "\n",
        "**L1 regularization with gradient descent optimizer:**\n",
        "\n",
        "1. Lambda: 1, train accuracy: 0.9966, test accuracy: 0.9976\n",
        "\n",
        "1. Lambda: 0.1, train accuracy: 0.9959, test accuracy: 0.9976\n",
        "\n",
        "2. Lambda: 0.001, train accuracy: 0.9964, test accuracy: 0.9970\n",
        "\n",
        "\n",
        "\n",
        "**Conclusion:** In L1 regularization, the hyperparameter lambda controls the strength of regularization. A higher lambda leads to stronger regularization, which can prevent overfitting but can also lead to underfitting. From the results above, we can see that a lower lambda value of 0.1 leads to a slightly better test accuracy than a lambda value of 1.\n",
        "\n",
        "\n",
        "\n",
        "**Mini-batch gradient descent:**\n",
        "\n",
        "1.  Batch size: 32, train accuracy: 0.9988, test accuracy: 0.9990\n",
        "\n",
        "1.  Batch size: 64, train accuracy: 0.9987, test accuracy: 0.9990\n",
        "\n",
        "2.  Batch size: 128, train accuracy: 0.9987, test accuracy: 0.9990\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Conclusion:** In mini-batch gradient descent, the batch size controls the number of examples used in each iteration of gradient descent. A larger batch size can provide a more accurate estimate of the gradient but can also increase the computational cost. From the results above, we can see that the batch size does not have a significant effect on the accuracy, with all batch sizes leading to high test accuracies.\n",
        "\n",
        "\n",
        "\n",
        "**RMS Prop optimizer:**\n",
        "\n",
        "\n",
        "1.  train accuracy: 0.9970, test accuracy: 0.9963\n",
        "\n",
        "**Conclusion:** In RMS Prop optimizer, the learning rate is automatically adjusted based on the gradient magnitudes. This can lead to faster convergence and better performance, especially when the gradient magnitudes are highly variable. From the results above, we can see that RMS Prop performs worse than other optimizers in terms of accuracy.\n",
        "\n",
        "**Adam optimizer:**\n",
        "\n",
        "1.  train accuracy: 0.9989, test accuracy: 0.9980\n",
        "\n",
        "**Conclusion:** In Adam optimizer, both the learning rate and the momentum are automatically adjusted based on the gradient magnitudes. This can provide even faster convergence and better performance than RMS Prop. From the results above, we can see that Adam performs better than other optimizers in terms of accuracy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
