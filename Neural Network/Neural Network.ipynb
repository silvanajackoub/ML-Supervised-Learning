{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "Jgz87dCgld-5"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NxyI7-NO9PdM"
      },
      "source": [
        "## Load MNIST Dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8z1tKillvs4",
        "outputId": "7fdef903-9fdd-4d1f-de10-97c3d7937d5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the MNIST dataset using fetch_openml\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "\n",
        "# Extract the features (pixel values) and labels from the dataset\n",
        "X = mnist.data.values.astype('float32')\n",
        "y = mnist.target.values.astype('int64')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GnLGAmSw9PdQ"
      },
      "source": [
        "## Standardize Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "i4opo_ggl0zP"
      },
      "outputs": [],
      "source": [
        "# Define a small epsilon value to add to the standard deviation to avoid division by zero\n",
        "eps = 1e-8\n",
        "\n",
        "# Calculate the standard deviation of each feature and replace any zero values with eps\n",
        "std_dev = np.std(X, axis=0)\n",
        "std_dev[std_dev == 0] = eps\n",
        "\n",
        "# Normalize the data by subtracting the mean and dividing by the standard deviation\n",
        "X = (X - np.mean(X, axis=0)) / std_dev"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IXmfNLxU9PdS"
      },
      "source": [
        "## Split Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "M8AMoIAwjMRw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(X , y ,test_size=0.2, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OQWmLRMP9PdT"
      },
      "source": [
        "## Apply one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "F9KBjc3RrYSD"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "# Convert y_train to one-hot encoded vectors\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "\n",
        "# Convert y_test to one-hot encoded vectors\n",
        "y_test = to_categorical(y_test, num_classes=10)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xoBkeyeD9PdV"
      },
      "source": [
        "## Implement Dynamic Neural Network from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "QUr2Emd6V2pC"
      },
      "outputs": [],
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "# Derivative of sigmoid activation function\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Mean Squared Error (MSE) loss function\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Neural Network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, x, y ,num_of_layers, size_of_layers):\n",
        "\n",
        "        num_features = x.shape[1]\n",
        "\n",
        "        size_of_layers.insert(0, num_features)\n",
        "        # Size (number of neurons) of each layer \n",
        "        self.size_of_layers = size_of_layers\n",
        "\n",
        "        # Number of layers in the network\n",
        "        num_of_layers = len(size_of_layers) \n",
        "        self.num_of_layers = num_of_layers\n",
        "       \n",
        "        \n",
        "        # Initialize random weights between layers\n",
        "        self.weights = [np.random.randn(size_of_layers[i], size_of_layers[i+1]) for i in range(num_of_layers-1)]\n",
        "\n",
        "        # Initialize random biases for each layer\n",
        "        self.biases = [np.random.randn(size_of_layers[i+1]) for i in range(num_of_layers-1)]\n",
        "\n",
        "        # Store the outputs of each layer\n",
        "        self.outputs = [np.zeros(size) for size in size_of_layers]\n",
        "\n",
        "        # Store the errors in each layer (excluding the input layer)\n",
        "        self.errors = [np.zeros(size) for size in size_of_layers[1:]]\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        # Set the input layer output as the input data\n",
        "        self.outputs[0] = x  \n",
        "\n",
        "        for i in range(self.num_of_layers-1):\n",
        "\n",
        "            # Compute the weighted sum of inputs and biases\n",
        "            z = np.dot(self.outputs[i], self.weights[i]) + self.biases[i]\n",
        "\n",
        "            # Apply the sigmoid activation function\n",
        "            self.outputs[i+1] = sigmoid(z)  \n",
        "\n",
        "            # Return the output of the last layer\n",
        "        return self.outputs[-1]  \n",
        "    \n",
        "    def backward(self, x, y_true, learning_rate):\n",
        "      num_samples = x.shape[0]\n",
        "      \n",
        "      # Perform forward propagation to get the predicted output\n",
        "      y_pred = self.forward(x)\n",
        "      \n",
        "      # Compute the error between predicted and true output\n",
        "      error = y_pred - y_true\n",
        "      \n",
        "      # Compute the delta (error * derivative of sigmoid)\n",
        "      delta = error * sigmoid_derivative(y_pred)\n",
        "      \n",
        "      for i in range(self.num_of_layers - 2, -1, -1):\n",
        "          # Store the error in the current layer\n",
        "          self.errors[i] = delta\n",
        "          \n",
        "          # Compute the delta for the previous layer using the weights and derivative of sigmoid\n",
        "          delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(self.outputs[i])\n",
        "          \n",
        "          # Compute the gradient of weights using outer product of outputs and errors\n",
        "          grad_w = np.dot(self.outputs[i].T, self.errors[i]) / num_samples\n",
        "          \n",
        "          # Gradient of biases is equal to the mean of errors along the samples\n",
        "          grad_b = np.mean(self.errors[i], axis=0)\n",
        "          \n",
        "          # Update the weights using the learning rate and gradient\n",
        "          self.weights[i] -= learning_rate * grad_w\n",
        "          \n",
        "          # Update the biases using the learning rate and gradient\n",
        "          self.biases[i] -= learning_rate * grad_b\n",
        "\n",
        "    def train(self, X_train, y_train, epochs, learning_rate):\n",
        "      \n",
        "      # Convert one-hot encoded labels to class indices\n",
        "      y_train_indices = np.argmax(y_train, axis=1)\n",
        "      for epoch in range(epochs):\n",
        "\n",
        "        # Perform backward propagation and update weights and biases for training set\n",
        "        self.backward(X_train, y_train, learning_rate)\n",
        "              \n",
        "        if epoch % 100 == 0:\n",
        "          \n",
        "          # Compute the mean squared error loss for the training set\n",
        "          train_loss = mse(y_train, self.forward(X_train))\n",
        "          print(f\"Epoch {epoch}: Train Loss = {train_loss:}\")\n",
        "          \n",
        "      train_accuracy = self.accuracy(X_train, y_train_indices)\n",
        "      # Compute the training accuracy\n",
        "      print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \n",
        "        # Perform forward propagation and round the predicted output to the nearest integer (0 or 1)\n",
        "        return self.forward(X)\n",
        "\n",
        "        \n",
        "     \n",
        "    def accuracy(self, X, y):\n",
        "        # Compute the predicted labels \n",
        "        predictions = self.predict(X)\n",
        "        predicted_labels = np.argmax(predictions, axis=1)\n",
        "        \n",
        "        # Compare predicted labels with true labels and calculate accuracy\n",
        "        accuracy = np.mean(predicted_labels == y)\n",
        "        return accuracy\n",
        "\n",
        "        \n",
        "\n",
        "def NN(x, y, num_of_layers, size_of_layers):\n",
        "  \n",
        "    nn = NeuralNetwork(x ,y ,num_of_layers=num_of_layers, size_of_layers=size_of_layers)\n",
        "    nn.train(x, y, epochs=1000, learning_rate=0.1)\n",
        "    return nn\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g9HGd6h3iHmq"
      },
      "source": [
        "### **Build NN with only 2 layers => 1 hidden layer and 1 output layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzC0LRM8zq-W",
        "outputId": "4f3045f8-9f72-438a-a502-16bde007c59a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Train Loss = 0.43856897766538727\n",
            "Epoch 100: Train Loss = 0.11709328987408103\n",
            "Epoch 200: Train Loss = 0.0842716827041173\n",
            "Epoch 300: Train Loss = 0.07257303730722137\n",
            "Epoch 400: Train Loss = 0.06574595333475419\n",
            "Epoch 500: Train Loss = 0.061005248455319475\n",
            "Epoch 600: Train Loss = 0.05740056799562045\n",
            "Epoch 700: Train Loss = 0.0544838112052316\n",
            "Epoch 800: Train Loss = 0.05206231775944955\n",
            "Epoch 900: Train Loss = 0.050014203375766834\n",
            "Train Accuracy: 0.7224\n"
          ]
        }
      ],
      "source": [
        "# Test NN function on MNIST dataset\n",
        "nn1 = NN(X_train, y_train, num_of_layers=2, size_of_layers=[20 , 10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "esP4C-jDWdWn"
      },
      "outputs": [],
      "source": [
        "# Convert y_test to class indices\n",
        "y_test_indices = np.argmax(y_test, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZxvw2vg9mLW",
        "outputId": "bc417fd6-0e07-4dac-88ab-4448ad69e31d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy1: 0.7246428571428571\n"
          ]
        }
      ],
      "source": [
        "# Calculate accuracy\n",
        "accuracy1_test = nn1.accuracy(X_test, y_test_indices)\n",
        "print(\"Test Accuracy1:\", accuracy1_test)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0qYpV-kMiYQg"
      },
      "source": [
        "### **Build NN with 3 layers=> 2 hidden layers**\n",
        "\n",
        "### **Where # of neurons in first layer < # of neurons in second layer and 1 output layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlIrzIPbbpak",
        "outputId": "bba6ef42-7f35-4609-c787-cc73af0a26f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Train Loss = 0.3919797573403472\n",
            "Epoch 100: Train Loss = 0.08473143500477898\n",
            "Epoch 200: Train Loss = 0.0712501849963578\n",
            "Epoch 300: Train Loss = 0.06415244041564808\n",
            "Epoch 400: Train Loss = 0.059389348819470535\n",
            "Epoch 500: Train Loss = 0.0557346709122797\n",
            "Epoch 600: Train Loss = 0.05292283591517028\n",
            "Epoch 700: Train Loss = 0.05068654135837108\n",
            "Epoch 800: Train Loss = 0.04882595510990051\n",
            "Epoch 900: Train Loss = 0.047200784350509464\n",
            "Train Accuracy: 0.7250\n"
          ]
        }
      ],
      "source": [
        "nn2 = NN(X_train, y_train, num_of_layers=3, size_of_layers=[20, 30, 10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ9fO0aWWqjH",
        "outputId": "88e1f298-e716-4b0c-ca51-48034d1e1ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy2 : 0.7225714285714285\n"
          ]
        }
      ],
      "source": [
        "# Calculate accuracy\n",
        "accuracy2_test = nn2.accuracy(X_test, y_test_indices)\n",
        "print(\"Test Accuracy2 :\", accuracy2_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bGxK-wW9i3R9"
      },
      "source": [
        "### **Build NN with 3 layers=> 2 hidden layers**\n",
        "\n",
        "### **Where # of neurons in first layer > # of neurons in second layer and 1 output layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwXPL8qVbpxe",
        "outputId": "2fb51bc7-b2e3-4647-bef3-7de61400df9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Train Loss = 0.5195187352012893\n",
            "Epoch 100: Train Loss = 0.09305800398601408\n",
            "Epoch 200: Train Loss = 0.07870458083586397\n",
            "Epoch 300: Train Loss = 0.07324613175091908\n",
            "Epoch 400: Train Loss = 0.06956975985748093\n",
            "Epoch 500: Train Loss = 0.06670985225595152\n",
            "Epoch 600: Train Loss = 0.06431042092080448\n",
            "Epoch 700: Train Loss = 0.06224003279411412\n",
            "Epoch 800: Train Loss = 0.060470403536967074\n",
            "Epoch 900: Train Loss = 0.058941710734149146\n",
            "Train Accuracy: 0.6465\n"
          ]
        }
      ],
      "source": [
        "nn3 = NN(X_train, y_train, num_of_layers=3, size_of_layers=[30, 20, 10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjKTO6TFWoqN",
        "outputId": "e0051f0e-8aae-4317-b4fe-9e5b0212c542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy3 : 0.6541428571428571\n"
          ]
        }
      ],
      "source": [
        "# Calculate accuracy\n",
        "accuracy3_test = nn3.accuracy(X_test, y_test_indices)\n",
        "print(\"Test Accuracy3 :\", accuracy3_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
